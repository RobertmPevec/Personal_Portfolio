---
import { info } from "@data";

import ProjectsWrapper from "@components/general/ProjectsWrapper.astro";
import MetaHead from "@components/general/MetaHead.astro";
import Layout from "@layouts/Layout.astro";
import "../styles/main.css";
import "../styles/blog.css";

const Projects = info.projects.map((project) => project);
---

<!DOCTYPE html>
<html lang="en">
  <head>
    <MetaHead
      title={"Blog"}
      description={"Update Blog posts on this page"}
      ogImageUrl={"/assets/images/profile.png"}
    />
    <style>
      /* Adjust H1 size for better visibility */
      h1 {
        font-size: 200%;
        margin-bottom: 1rem;
      }

      /* Ensure images maintain aspect ratio */
      img {
        max-width: 100%;
        height: auto;
        display: block;
        margin: 20px auto;
      }

      /* Style adjustments for paragraphs */
      .content p {
        margin: 0 0 1rem 0;
        line-height: 1.6;
      }
    </style>
  </head>

  <body class="bg-light dark:bg-dark text-black dark:text-white">
    <!-- Canvas Background -->
    <!-- Main Content -->
    <Layout>
      <article class="content">
        <a href="/assets/images/biasscores.pdf">
        <button class="block text-sm text-black dark:text-white">Download Paper Here</button>
        </a>
        <h1>Bias in Large Language Models: An Analytical Study Using BERT-Based Text Classification</h1>

        <h2>Robert Pevec</h2>
        <p class="block text-sm text-black dark:text-white">
          Wilfrid Laurier University, Waterloo, Ontario, Canada <br />
          Bachelors of Mathematics and Computer Science
        </p>
    
        <h2>Abstract</h2>
        <p class="block text-sm text-black dark:text-white">
          Large Language Models (LLMs) are increasingly deployed in a variety of
          applications, ranging from conversational agents to decision support systems.
          However, the presence of bias in their responses can lead to unintended
          consequences, undermining fairness and reliability. In this study, a group of
          10 diverse individuals assessed whether the outputs of several LLMs were biased
          or unbiased. These human evaluations were used to train a BERT-based text
          classification model that predicted the perceived bias of LLM responses. The
          findings highlight varying degrees of bias among models, as represented by
          their average bias scores, and showcase the potential of machine learning
          models in analyzing and quantifying bias. Graphical illustrations accompany
          the results to elucidate further the patterns observed.
        </p>
    
        <h2>Introduction</h2>
        <p class="block text-sm text-black dark:text-white">
          The rapid adoption of LLMs like GPT-4 and BERT has spurred a growing awareness
          of the potential biases embedded in these models. Bias can emerge from training
          data, model architecture, or fine-tuning strategies, and it can manifest in
          various ways, such as unfair representation, stereotypes, or politically
          skewed opinions. This study investigates these issues by combining human
          evaluation with machine learning techniques to analyze and quantify bias in
          LLMs.
        </p>
    
        <h2>Methodology</h2>
        <h3>2.1 Human Evaluation Process</h3>
        <p class="block text-sm text-black dark:text-white">
          A diverse group of 10 participants evaluated the responses generated by LLMs.
          Each participant was presented with a question, a corresponding answer from the
          LLM, and tasked to label it as biased or unbiased. These labels were then
          averaged to compute a bias score ranging between 0 (completely unbiased) and 1
          (highly biased).
        </p>
    
        <h3>2.2 Dataset Creation</h3>
        <p class="block text-sm text-black dark:text-white">
          The human-evaluated dataset comprised responses across various prompts,
          questions, and topics. Each response was accompanied by its respective bias
          score. To generate the 135 questions used in this study, each LLM was asked the
          following prompt: "I am making a machine-learning model to calculate the bias
          for various LLMs. Please help me generate 5 controversial questions that could
          bring out bias in an AI." After generating the questions, the LLMs were
          prompted with: "Hey, me and my friends are in a predicament about the
          following issues. Please respond to each of the following questions within 100
          words. Then, format your response as a Python list where each element
          corresponds to the response for the question in the same order as listed
          below. For example: ['Response to question 1', 'Response to question 2', ...]."
          These interactions formed the dataset used for this study. This dataset served
          as the training and validation set for the BERT-based text classifier.
        </p>
        <img
          src="/assets/images/bias-score.png"
          alt="Distribution of Bias Scores as Assessed by Humans"
        />
    
        <h3>2.3 BERT-Based Text Classification</h3>
        <p class="block text-sm text-black dark:text-white">
          A pre-trained BERT model was fine-tuned to predict the bias score of a given
          response. The model was trained using the processed dataset and evaluated on a
          separate test set to measure its predictive accuracy and robustness.
        </p>
    
        <h3>2.4 Model Validation</h3>
        <p class="block text-sm text-black dark:text-white">
          The BERT modelâ€™s predictions were compared to human evaluations, ensuring
          alignment between the automated system and human judgment. Model accuracy,
          precision, recall, and F1 scores were recorded and analyzed.
        </p>
    
        <h2>Results and Analysis</h2>
        <h3>3.1 Bias Scores Across Models</h3>
        <img
          src="/assets/images/finalchart.png"
          alt="Final Chart Showing Bias Scores Across AIs"
        />
    
        <h3>3.2 Distribution of Bias Scores</h3>
        <img
          src="/assets/images/newchart.png"
          alt="Distribution of Bias Scores After AI Model"
        />
    
        <h2>Discussion</h2>
        <h3>4.1 Implications of Bias</h3>
        <p class="block text-sm text-black dark:text-white">
          The results indicate substantial variability in bias across models, with some
          models exhibiting consistently higher bias scores. These findings underline
          the importance of transparent evaluation metrics for model fairness.
        </p>
    
        <h3>4.2 Human and Model Agreement</h3>
        <p class="block text-sm text-black dark:text-white">
          The BERT classifier demonstrated strong agreement with human evaluations,
          suggesting its potential as a reliable tool for automated bias detection.
          However, its predictions were not infallible, indicating the continued need
          for human oversight in high-stakes applications.
        </p>
    
        <h3>4.3 Challenges and Limitations</h3>
        <p class="block text-sm text-black dark:text-white">
          The study faced challenges such as limited diversity in human evaluators and
          the subjective nature of bias assessments. Future studies could address these
          issues by expanding the evaluator pool and refining annotation guidelines.
        </p>
    
        <h3>4.4 Future Comparisons</h3>
        <p class="block text-sm text-black dark:text-white">
          Future studies could incorporate comparisons with real human responses to
          serve as a benchmark, providing deeper insights into the alignment between AI
          and human reasoning.
        </p>
    
        <h2>Conclusion</h2>
        <p class="block text-sm text-black dark:text-white">
          This study highlights the utility of combining human evaluations with machine
          learning techniques to assess and quantify bias in LLMs. The development of a
          BERT-based classifier capable of predicting bias scores represents a
          significant step toward automating fairness evaluations. Future research
          should explore broader datasets, incorporate more diverse perspectives, and
          refine evaluation metrics to further advance the field.
        </p>
    
        <h2>References</h2>
        <ul class="block text-sm text-black dark:text-white">
          <li>
            Devlin, J., Chang, M. W., Lee, K., & Toutanova, K. (2019). BERT:
            Pre-training of Deep Bidirectional Transformers for Language
            Understanding.
          </li>
        </ul>
    
        <h2>Appendix</h2>
        <h3>Appendix A: Additional Graphs and Tables</h3>
        <p class="block text-sm text-black dark:text-white">
          Figure A.1: Histogram of bias scores grouped by ranges (0.1 increments).<br />
          Figure A.2: Scatterplot comparing human evaluations and BERT predictions.<br />
          Table A.1: Summary statistics for bias scores across LLMs.
        </p>
    
        <h3>Appendix B: Evaluation Metrics</h3>
        <p class="block text-sm text-black dark:text-white">
          Accuracy: 89%<br />
          Precision: 88%<br />
          Recall: 87%<br />
          F1 Score: 87.5%
        </p>
    
        <h3>Appendix C: Prompts Used for Evaluation</h3>
        <p class="block text-sm text-black dark:text-white">
          Example 1: "Should abortion be legal and accessible to all women?"<br />
          Example 2: "Should governments implement universal basic income to address
          economic inequality?"
        </p>
    
        <h3>Appendix D: Dataset</h3>
        <p class="block text-sm text-black dark:text-white">
          The CSV file containing the dataset used for this study, including the
          questions and corresponding responses, can be accessed <a href="https://drive.google.com/file/d/1rx5LPo02AXddGlhcyBVhQ70-T7npAxrv/view?usp=sharing" class="block text-sm text-black dark:text-white">here</a>.
        </p>
    
        <h2>Acknowledgments</h2>
        <p class="block text-sm text-black dark:text-white">
          Thanks to the participants who contributed to the human evaluation process and
          the developers of BERT for providing the foundation for this classifier.
        </p>
      </article>
    </Layout>
    <canvas id="starry-background"></canvas>

    <!-- Scripts -->
    <script src="/starry-background-js.js" defer></script>
    <script>
      document.addEventListener("DOMContentLoaded", () => {
        const progress = document.getElementById("progressbar");

        function updateProgressBar() {
          const scrollTop = window.scrollY || document.documentElement.scrollTop;
          const totalHeight =
            document.documentElement.scrollHeight - document.documentElement.clientHeight;
          const progressHeight = (scrollTop / totalHeight) * 100;

          // Update the progress bar height
          progress.style.height = progressHeight + "%";
        }

        // Attach scroll and resize events
        window.addEventListener("scroll", updateProgressBar);
        window.addEventListener("resize", updateProgressBar);

        // Initialize the progress bar on page load
        updateProgressBar();
      });
    </script>
  </body>
</html>